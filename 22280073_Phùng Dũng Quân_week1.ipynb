{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "Vietnamese Students' Feedback Corpus (UIT-VSFC) is the resource consists of over 16,000 sentences which are human-annotated with two different tasks: sentiment-based and topic-based classifications.\n",
    "\n",
    "[1] Kiet Van Nguyen, Vu Duc Nguyen, Phu Xuan-Vinh Nguyen, Tham Thi-Hong Truong, Ngan Luu-Thuy Nguyen, UIT-VSFC: Vietnamese Students' Feedback Corpus for Sentiment Analysis,  2018 10th International Conference on Knowledge and Systems Engineering (KSE 2018), November 1-3, 2018, Ho Chi Minh City, Vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"uitnlp/vietnamese_students_feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentiment', 'topic'],\n",
       "    num_rows: 11426\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = dataset['train']\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'slide giáo trình đầy đủ .', 'sentiment': 2, 'topic': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11426"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a sentence\n",
    "example_word_list = train_set[0]['sentence']\n",
    "example_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide', 'giáo', 'trình', 'đầy', 'đủ', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sentence word-by-word\n",
    "example_word_list.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slide giáo trình đầy đủ .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join words into 1 full sentence\n",
    "sentence = \"\"\n",
    "for word in example_word_list:\n",
    "    sentence += word\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slide giáo trình đầy đủ .',\n",
       " 'nhiệt tình giảng dạy , gần gũi với sinh viên .',\n",
       " 'đi học đầy đủ full điểm chuyên cần .',\n",
       " 'chưa áp dụng công nghệ thông tin và các thiết bị hỗ trợ cho việc giảng dạy .',\n",
       " 'thầy giảng bài hay , có nhiều bài tập ví dụ ngay trên lớp .',\n",
       " 'giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .',\n",
       " 'em sẽ nợ môn này , nhưng em sẽ học lại ở các học kỳ kế tiếp .',\n",
       " 'thời lượng học quá dài , không đảm bảo tiếp thu hiệu quả .',\n",
       " 'nội dung môn học có phần thiếu trọng tâm , hầu như là chung chung , khái quát khiến sinh viên rất khó nắm được nội dung môn học .',\n",
       " 'cần nói rõ hơn bằng cách trình bày lên bảng thay vì nhìn vào slide .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 sentences to process\n",
    "sentence_list = []\n",
    "for idx in range(10):\n",
    "    sentence = \"\"\n",
    "    for word in train_set[idx]['sentence']:\n",
    "        sentence += word\n",
    "    sentence_list.append(sentence)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "- N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document.\n",
    "- We can use n-grams or multiple other text preprocessing algorithms by incorporating [`nltk`](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This document is the second document.\n",
      "===============\n",
      "1-gram: ['This', 'document', 'is', 'the', 'second', 'document.']\n",
      "\n",
      "2-gram: ['This document', 'document is', 'is the', 'the second', 'second document.']\n",
      "\n",
      "3-gram: ['This document is', 'document is the', 'is the second', 'the second document.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "\n",
    "num_of_grams = np.arange(1, 4, 1) # Test 3 n-grams\n",
    "\n",
    "print(\"Original sentence:\", example_sentence[1])\n",
    "print(\"===\"*5)\n",
    "\n",
    "for gram in num_of_grams:\n",
    "    splitted_sentence = ngrams(example_sentence[1].split(), int(gram))\n",
    "    print(f\"{gram}-gram: \",end ='')\n",
    "    n_grams_list = [' '.join(grams) for grams in splitted_sentence]\n",
    "    print(n_grams_list)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1  2  3\n",
       "and       0  0  1  0\n",
       "document  1  2  0  1\n",
       "first     1  0  0  1\n",
       "is        1  1  1  1\n",
       "one       0  0  1  0\n",
       "second    0  1  0  0\n",
       "the       1  1  1  1\n",
       "third     0  0  1  0\n",
       "this      1  1  1  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: giảng viên đảm bảo thời gian lên lớp , tích cực trả lời câu hỏi của sinh viên , thường xuyên đặt câu hỏi cho sinh viên .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bảo</th>\n",
       "      <th>cho</th>\n",
       "      <th>câu</th>\n",
       "      <th>của</th>\n",
       "      <th>cực</th>\n",
       "      <th>gian</th>\n",
       "      <th>giảng</th>\n",
       "      <th>hỏi</th>\n",
       "      <th>lên</th>\n",
       "      <th>lớp</th>\n",
       "      <th>lời</th>\n",
       "      <th>sinh</th>\n",
       "      <th>thường</th>\n",
       "      <th>thời</th>\n",
       "      <th>trả</th>\n",
       "      <th>tích</th>\n",
       "      <th>viên</th>\n",
       "      <th>xuyên</th>\n",
       "      <th>đảm</th>\n",
       "      <th>đặt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bảo  cho  câu  của  cực  gian  giảng  hỏi  lên  lớp  lời  sinh  thường  \\\n",
       "0    1    1    2    1    1     1      1    2    1    1    1     2       1   \n",
       "\n",
       "   thời  trả  tích  viên  xuyên  đảm  đặt  \n",
       "0     1    1     1     3      1    1    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform([sentence_list[5]]).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "print('Example sentence:', sentence_list[5])\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document is</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first document</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is this</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the first</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the second</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1  2  3\n",
       "and              0  0  1  0\n",
       "and this         0  0  1  0\n",
       "document         1  2  0  1\n",
       "document is      0  1  0  0\n",
       "first            1  0  0  1\n",
       "first document   1  0  0  1\n",
       "is               1  1  1  1\n",
       "is the           1  1  1  0\n",
       "is this          0  0  0  1\n",
       "one              0  0  1  0\n",
       "second           0  1  0  0\n",
       "second document  0  1  0  0\n",
       "the              1  1  1  1\n",
       "the first        1  0  0  1\n",
       "the second       0  1  0  0\n",
       "the third        0  0  1  0\n",
       "third            0  0  1  0\n",
       "third one        0  0  1  0\n",
       "this             1  1  1  1\n",
       "this document    0  1  0  0\n",
       "this is          1  0  1  0\n",
       "this the         0  0  0  1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize_model = CountVectorizer(ngram_range = (1, 2))\n",
    "\n",
    "n_grams_feature_vector = count_vectorize_model.fit_transform(example_sentence).toarray()\n",
    "\n",
    "word_frequency = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "word_frequency.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1\n",
    "Based on the UIT-VSFC dataset and the aforementioned information.\n",
    "- Create an $n$-gram word frequency table, such that $n$ could be any number of your desire.\n",
    "- With $n=1$ and $n=2$, what is the most popular word in the dataset ?\n",
    "- With $n=1$ and $n=2$, what is the rarest word in the dataset ?\n",
    "- What are the limitations of this data processing flow ? How can we overcome those ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all sentences within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_all_sentences(dataset) -> List[str]:\n",
    "    \"\"\"\n",
    "    Function to get all sentences and store them into a list of strings\n",
    "\n",
    "    Args:\n",
    "    dataset -- The subset (i.e., train/valid/test) in UIT-VSFC dataset\n",
    "\n",
    "    Returns:\n",
    "    A list of all sentences in a subset data of the UIT-VSFC.\n",
    "    \"\"\"\n",
    "\n",
    "    list_all_sentence: list = []\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    for idx in range(len(dataset)):\n",
    "        sentence = \"\"\n",
    "        for word in dataset[idx]['sentence']:\n",
    "            sentence += word\n",
    "        list_all_sentence.append(sentence)\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return list_all_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences within the dataset: 11426\n",
      "Example sentence: slide giáo trình đầy đủ .\n"
     ]
    }
   ],
   "source": [
    "list_all_sentence: list = get_all_sentences(train_set)\n",
    "print(f\"#sentences within the dataset: {len(list_all_sentence)}\")\n",
    "print(f\"Example sentence: {list_all_sentence[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the word frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_word_frequency(sentence_list: list,\n",
    "                          n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to build a word frequency table based on n-grams\n",
    "\n",
    "    Args:\n",
    "    sentence_list (list) -- A list of all sentences needed for table constructing process\n",
    "    n (int) -- Number of grams that we parse into this function\n",
    "\n",
    "    Returns:\n",
    "    A dataframe contains all words after conducting n-grams and their respective frequencies\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "\n",
    "    count_vectorize_model = CountVectorizer(ngram_range = (n, n))\n",
    "    n_grams_feature_vector = count_vectorize_model.fit_transform(sentence_list).toarray()\n",
    "    word_frequency_table = pd.DataFrame(data = n_grams_feature_vector, columns = count_vectorize_model.get_feature_names_out())\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return word_frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Frequency\n",
      "viên        4803\n",
      "giảng       3711\n",
      "dạy         3156\n",
      "thầy        3095\n",
      "sinh        3082\n",
      "học         2940\n",
      "bài         2336\n",
      "tình        2266\n",
      "không       2177\n",
      "và          2068\n",
      "Từ phổ biến nhất:\n",
      " viên\n",
      "               Frequency\n",
      "dọa                    1\n",
      "đếm                    1\n",
      "đế                     1\n",
      "gán                    1\n",
      "ấm                     1\n",
      "ướt                    1\n",
      "ức                     1\n",
      "đống                   1\n",
      "đốn                    1\n",
      "11doubledot55          1\n",
      "Từ hiếm gặp nhất:\n",
      " 11doubledot55\n"
     ]
    }
   ],
   "source": [
    "# Construct the table of word frequency\n",
    "# 1-ngram\n",
    "word_frequency_table_1ngram = n_gram_word_frequency(sentence_list=list_all_sentence,\n",
    "                                             n=1)\n",
    "word_frequency_1ngram = word_frequency_table_1ngram.sum(axis=0).sort_values(ascending=False).to_frame('Frequency')\n",
    "\n",
    "print(word_frequency_1ngram[:10])\n",
    "print(\"Từ phổ biến nhất:\\n\", word_frequency_1ngram.index[0])\n",
    "print(word_frequency_1ngram[-10:])\n",
    "print(\"Từ hiếm gặp nhất:\\n\", word_frequency_1ngram.index[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Frequency\n",
      "sinh viên        2698\n",
      "nhiệt tình       1848\n",
      "giảng viên       1610\n",
      "bài tập          1057\n",
      "dễ hiểu          1004\n",
      "giảng dạy         956\n",
      "kiến thức         904\n",
      "thực hành         877\n",
      "môn học           688\n",
      "cho sinh          656\n",
      "Từ phổ biến nhất:\n",
      " sinh viên\n",
      "                  Frequency\n",
      "100 cách                  1\n",
      "100 là                    1\n",
      "100 người                 1\n",
      "100 tự                    1\n",
      "10h mới                   1\n",
      "10h30 nhưng               1\n",
      "11 thì                    1\n",
      "11doubledot55 pm          1\n",
      "11h30 nghỉ                1\n",
      "trên google               1\n",
      "Từ hiếm gặp nhất:\n",
      " trên google\n"
     ]
    }
   ],
   "source": [
    "# 2-ngram\n",
    "word_frequency_table_2ngram = n_gram_word_frequency(sentence_list=list_all_sentence,\n",
    "                                             n=2)\n",
    "word_frequency_2ngram = word_frequency_table_2ngram.sum(axis=0).sort_values(ascending=False).to_frame('Frequency')\n",
    "\n",
    "print(word_frequency_2ngram[:10])\n",
    "print(\"Từ phổ biến nhất:\\n\", word_frequency_2ngram.index[0])\n",
    "print(word_frequency_2ngram[-10:])\n",
    "print(\"Từ hiếm gặp nhất:\\n\", word_frequency_2ngram.index[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should comment your answer to problem 1 here with sufficient explanations, including your implementation and reasoning.\n",
    "\n",
    "- With n = 1, the most popular word is \"viên\" appearing 4803 times, the rarest word is \"11doubledot55\" appearing 1 time. However, there are still many words appearing once. Eg: \"đốn\", \"đống\", \"ức\".\n",
    "- With n = 2, the most popular word is \"sinh viên\" appearing 2698 times, the rarest word is \"trên google\" appearing 1 time. However, there are still many words appearing once. Eg: \"11h30 nghỉ\", \"11doubledot55 pm\", \"11 thì\".\n",
    "- The limitations of this data processing flow is stopwords may take many quantity, it will reduce the important of remaining words though they are really needed.\n",
    "- Mispelling is also a problem because it will disperse frequency of words. Eg: \"sinhviên\" and \"sinh viên\" may be separated count with different meaning.\n",
    "- The computation cost will be large if the 'n' large. It also take much time to compute.\n",
    "- If sentence appear many words only once, it will lead to data sparsity. Use Laplace smoothing to assign a small probability to rare words.\n",
    "- Using higher n-grams or embeddings to fix Loss Context.\n",
    "- Using Laplace Smoothing to fix infrequent word case.\n",
    "- \"Các em học sinh đang học rất chăm chỉ\", từ \"học\" trong câu này có 2 ý nghĩa. Sử dụng word embeddings hoặc có thể áp dụng mô hình ngữ cảnh Bert để phân tích sâu hơn.\n",
    "- Using TF-IDF for imbalance data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the stopword dictionary\n",
    "import wget\n",
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of stop words: 1942\n"
     ]
    }
   ],
   "source": [
    "# Observe stopwords list\n",
    "vietnamese_stopword = open('vietnamese-stopwords.txt', 'r', encoding='utf-8').read()\n",
    "vietnamese_stopword = vietnamese_stopword.split('\\n') # Separate lines by lines\n",
    "print(f\"#Number of stop words: {len(vietnamese_stopword)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a lô\n",
      "a ha\n",
      "ai\n",
      "ai ai\n",
      "ai nấy\n",
      "ai đó\n",
      "alô\n",
      "amen\n",
      "anh\n",
      "anh ấy\n"
     ]
    }
   ],
   "source": [
    "# Stop words example\n",
    "for sentence in vietnamese_stopword[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - Invert document frequency (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "Term frequency (TF) is the number of times a given term appears in document\n",
    "\n",
    "$$\n",
    "tf(t) = f(t,d)\\times\\frac{1}{T}\n",
    "$$\n",
    "whereas, $f(t,d)$ is the frequency of the word $t$ in the document $d$, $T$ is the number of all words in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf\n",
       "and       0.0\n",
       "document  0.2\n",
       "first     0.2\n",
       "is        0.2\n",
       "one       0.0\n",
       "second    0.0\n",
       "the       0.2\n",
       "third     0.0\n",
       "this      0.2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Declare TF vectorize\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                use_idf=False, # only using TF\n",
    "                                norm='l1')\n",
    "\n",
    "tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_vectorized = tf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_output = tf_vectorized[0]\n",
    "\n",
    "# Build TF table\n",
    "words_tf_idf = pd.DataFrame(tf_output.T.todense(), index=tf_vectorizer.get_feature_names_out(), columns=['tf'])\n",
    "words_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "\n",
    "Inverse Document Frequency, or abbreviated as IDF, measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones.\n",
    "\n",
    "$$\n",
    "idf(t) = \\log\\left(\\frac{\\text{#documents in the document set}}{\\text{#documents with term}}\\right) + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf\n",
       "is        0.2  1.000000\n",
       "the       0.2  1.000000\n",
       "this      0.2  1.000000\n",
       "document  0.2  1.287682\n",
       "first     0.2  1.693147\n",
       "and       0.0  2.386294\n",
       "second    0.0  2.386294\n",
       "one       0.0  2.386294\n",
       "third     0.0  2.386294"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Configure settings for IDF vectorize\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm=None)\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Retrieve only idf information\n",
    "idf_vectorizer = tf_idf_vectorizer.idf_\n",
    "\n",
    "# Join idf values into the previous dataframe\n",
    "words_tf_idf['idf'] = idf_vectorizer\n",
    "\n",
    "# Show dataframe with ascending values of idf\n",
    "words_tf_idf.sort_values(by=['idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Technically saying, TF-IDF is a score which is applied to every word in every document in our dataset. And for every word, the TF-IDF value increases with every appearance of the word in a document, but is gradually decreased with every appearance in other documents\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}= tf(t, d) \\times idf(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.215302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.283096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf       idf    tf-idf\n",
       "and       0.0  2.386294  0.000000\n",
       "third     0.0  2.386294  0.000000\n",
       "second    0.0  2.386294  0.000000\n",
       "one       0.0  2.386294  0.000000\n",
       "is        0.2  1.000000  0.167201\n",
       "the       0.2  1.000000  0.167201\n",
       "this      0.2  1.000000  0.167201\n",
       "document  0.2  1.287682  0.215302\n",
       "first     0.2  1.693147  0.283096"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                    smooth_idf=False,\n",
    "                                    use_idf=True,\n",
    "                                    norm='l1')\n",
    "\n",
    "tf_idf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_idf_vectorized = tf_idf_vectorizer.transform(corpus)\n",
    "\n",
    "tf_idf_output = tf_idf_vectorized[0]\n",
    "words_tf_idf['tf-idf'] = tf_idf_output.T.todense()\n",
    "\n",
    "words_tf_idf.sort_values(by=['tf-idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem set 2\n",
    "Based on the problem 1 and the instruction on TF, IDF, TF-IDF:\n",
    "- (2a) Build the tf-idf table for the UIT-VSFC dataset with $n$-gram = 1 and $n$-gram = 2.\n",
    "- (2b) Change a few hyperparameters in the `TfidfVectorizer` function (`smooth_idf`, `sublinear_tf` and `norm`) from problem 2a (*you could browse from this [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to discover which are the correct paramters to parse*). Explain the results differences collected after modifying hyperparameters.\n",
    "- (2c) Which words has the lowest and the highest tf-idf values ? Do they differ from $n$-grams results ?\n",
    "- (2d) Which limitations from $n$-grams that TF-IDF overcame ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Build the tf-idf table for the UIT-VSFC dataset with n-gram = 1 and n-gram = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10h</th>\n",
       "      <th>10h30</th>\n",
       "      <th>11</th>\n",
       "      <th>11doubledot55</th>\n",
       "      <th>11h30</th>\n",
       "      <th>11h55</th>\n",
       "      <th>12</th>\n",
       "      <th>12doubledot00</th>\n",
       "      <th>...</th>\n",
       "      <th>ấy</th>\n",
       "      <th>ẩn</th>\n",
       "      <th>ắt</th>\n",
       "      <th>ốc</th>\n",
       "      <th>ồn</th>\n",
       "      <th>ổn</th>\n",
       "      <th>ủa</th>\n",
       "      <th>ủng</th>\n",
       "      <th>ức</th>\n",
       "      <th>ứng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100  10h  10h30   11  11doubledot55  11h30  11h55   12  12doubledot00  \\\n",
       "0  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "1  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "2  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "3  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "4  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "5  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "6  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "7  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "8  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "9  0.0  0.0  0.0    0.0  0.0            0.0    0.0    0.0  0.0            0.0   \n",
       "\n",
       "   ...   ấy   ẩn   ắt   ốc   ồn   ổn   ủa  ủng   ức  ứng  \n",
       "0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 2459 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1-gram\n",
    "tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_1gram = tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "tfidf_1gram_df = pd.DataFrame(tfidf_1gram.toarray(), columns=tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "tfidf_1gram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 2-gram:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 50</th>\n",
       "      <th>10 bài</th>\n",
       "      <th>10 fraction</th>\n",
       "      <th>10 kiến</th>\n",
       "      <th>10 luôn</th>\n",
       "      <th>10 mấy</th>\n",
       "      <th>10 mới</th>\n",
       "      <th>10 người</th>\n",
       "      <th>10 năm</th>\n",
       "      <th>10 phút</th>\n",
       "      <th>...</th>\n",
       "      <th>ứng kịp</th>\n",
       "      <th>ứng nhu</th>\n",
       "      <th>ứng nhưng</th>\n",
       "      <th>ứng tốt</th>\n",
       "      <th>ứng yêu</th>\n",
       "      <th>ứng đáp</th>\n",
       "      <th>ứng đúng</th>\n",
       "      <th>ứng được</th>\n",
       "      <th>ứng đầy</th>\n",
       "      <th>ứng đủ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 50  10 bài  10 fraction  10 kiến  10 luôn  10 mấy  10 mới  10 người  \\\n",
       "0    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "1    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "2    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "3    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "4    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "5    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "6    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "7    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "8    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "9    0.0     0.0          0.0      0.0      0.0     0.0     0.0       0.0   \n",
       "\n",
       "   10 năm  10 phút  ...  ứng kịp  ứng nhu  ứng nhưng  ứng tốt  ứng yêu  \\\n",
       "0     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "1     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "2     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "3     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "4     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "5     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "6     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "7     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "8     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "9     0.0      0.0  ...      0.0      0.0        0.0      0.0      0.0   \n",
       "\n",
       "   ứng đáp  ứng đúng  ứng được  ứng đầy  ứng đủ  \n",
       "0      0.0       0.0       0.0      0.0     0.0  \n",
       "1      0.0       0.0       0.0      0.0     0.0  \n",
       "2      0.0       0.0       0.0      0.0     0.0  \n",
       "3      0.0       0.0       0.0      0.0     0.0  \n",
       "4      0.0       0.0       0.0      0.0     0.0  \n",
       "5      0.0       0.0       0.0      0.0     0.0  \n",
       "6      0.0       0.0       0.0      0.0     0.0  \n",
       "7      0.0       0.0       0.0      0.0     0.0  \n",
       "8      0.0       0.0       0.0      0.0     0.0  \n",
       "9      0.0       0.0       0.0      0.0     0.0  \n",
       "\n",
       "[10 rows x 31384 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-ngram\n",
    "tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(2, 2))\n",
    "tfidf_2gram = tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "tfidf_2gram_df = pd.DataFrame(tfidf_2gram.toarray(), columns=tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF with 2-gram:\")\n",
    "tfidf_2gram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Change a few hyperparameters in the TfidfVectorizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 1-gram:\n",
      "           TF-IDF\n",
      "viên   659.324137\n",
      "giảng  658.334312\n",
      "dạy    616.080713\n",
      "tình   558.548969\n",
      "thầy   552.767907\n",
      "        TF-IDF\n",
      "tá    0.125334\n",
      "vạ    0.120081\n",
      "gạo   0.120081\n",
      "ùn    0.120081\n",
      "lõng  0.120081\n",
      "TF-IDF with 1-gram (new hyperparameters):\n",
      "           TF-IDF\n",
      "dạy    230.719080\n",
      "giảng  230.481666\n",
      "tình   211.496246\n",
      "viên   205.576416\n",
      "thầy   198.167823\n",
      "        TF-IDF\n",
      "hỏa   0.023056\n",
      "lõng  0.016674\n",
      "gạo   0.016674\n",
      "ùn    0.016674\n",
      "vạ    0.016674\n"
     ]
    }
   ],
   "source": [
    "# 1-ngram\n",
    "new_tfidf_vectorizer_1gram = TfidfVectorizer(ngram_range=(1, 1), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "new_tfidf_1gram = new_tfidf_vectorizer_1gram.fit_transform(list_all_sentence)\n",
    "new_tfidf_1gram_df = pd.DataFrame(new_tfidf_1gram.toarray(), columns=new_tfidf_vectorizer_1gram.get_feature_names_out())\n",
    "new_tfidf_sort_1gram = new_tfidf_1gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "tfidf_sort_1gram = tfidf_1gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "print(tfidf_sort_1gram.head(5))\n",
    "print(tfidf_sort_1gram[-5:])\n",
    "print(\"TF-IDF with 1-gram (new hyperparameters):\")\n",
    "print(new_tfidf_sort_1gram.head(5))\n",
    "print(new_tfidf_sort_1gram[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with 2-gram:\n",
      "                TF-IDF\n",
      "nhiệt tình  327.391256\n",
      "sinh viên   266.909184\n",
      "giảng viên  234.135877\n",
      "dễ hiểu     211.014566\n",
      "giảng dạy   179.107510\n",
      "            TF-IDF\n",
      "sức và    0.095209\n",
      "cả tên    0.095209\n",
      "lõng mơ   0.095209\n",
      "buộc thì  0.095209\n",
      "vạ gạo    0.095209\n",
      "TF-IDF with 2-gram (new hyperparameters):\n",
      "                TF-IDF\n",
      "nhiệt tình  134.238251\n",
      "dễ hiểu      84.110376\n",
      "giảng viên   82.846095\n",
      "sinh viên    78.990046\n",
      "giảng dạy    67.601236\n",
      "              TF-IDF\n",
      "buổi thông  0.008826\n",
      "mềm hệ      0.008826\n",
      "như phần    0.008826\n",
      "một vạ      0.008826\n",
      "gạo một     0.008826\n"
     ]
    }
   ],
   "source": [
    "# 2-ngram\n",
    "new_tfidf_vectorizer_2gram = TfidfVectorizer(ngram_range=(2, 2), smooth_idf=False, sublinear_tf=True, norm='l1')\n",
    "new_tfidf_2gram = new_tfidf_vectorizer_2gram.fit_transform(list_all_sentence)\n",
    "new_tfidf_2gram_df = pd.DataFrame(new_tfidf_2gram.toarray(), columns=new_tfidf_vectorizer_2gram.get_feature_names_out())\n",
    "new_tfidf_sort_2gram = new_tfidf_2gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "tfidf_sort_2gram = tfidf_2gram_df.sum(axis=0).sort_values(ascending=False).to_frame('TF-IDF')\n",
    "\n",
    "print(\"TF-IDF with 2-gram:\")\n",
    "print(tfidf_sort_2gram.head(5))\n",
    "print(tfidf_sort_2gram[-5:])\n",
    "print(\"TF-IDF with 2-gram (new hyperparameters):\")\n",
    "print(new_tfidf_sort_2gram.head(5))\n",
    "print(new_tfidf_sort_2gram[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the results differences collected after modifying hyperparameters.\n",
    "- Change 'smooth_idf' from True(default) into False to reduce weight of words:\n",
    "- - Mặc định, smooth_idf=True sẽ cộng thêm 1 vào số lần xuất hiện của mỗi từ trong công thức IDF để tránh lỗi chia cho 0 (nếu một từ chỉ xuất hiện trong một tài liệu duy nhất).\n",
    "- - Khi đặt smooth_idf=False, hệ số IDF sẽ không được làm trơn, khiến các từ hiếm có IDF cao hơn và các từ phổ biến có IDF thấp hơn một cách rõ ràng hơn.\n",
    "- Change 'sublinear_tf' from False(default) into True to reduce influence of word which appear many times in one document. Help to balance out their influence across different documents:\n",
    "- - Mặc định, sublinear_tf=False sử dụng giá trị TF (Term Frequency) trực tiếp, nghĩa là nếu một từ xuất hiện nhiều lần trong một tài liệu, trọng số của nó cũng sẽ tăng tương ứng.\n",
    "- - Khi đặt sublinear_tf=True, TF sẽ được thay đổi thành log(1 + tf), giúp giảm ảnh hưởng của các từ xuất hiện nhiều lần trong cùng một tài liệu.\n",
    "- Change 'norm' from l2(default) into l1, TF-IDF values sum to 1 per document instead of being scaled by Euclidean norm, which may affect how documents are compared:\n",
    "- - Mặc định, norm=l2 chuẩn hóa vector TF-IDF bằng chuẩn Euclid, giúp giữ độ dài vector đồng nhất giữa các tài liệu, điều này có lợi cho các mô hình dựa trên khoảng cách như SVM hoặc KNN.\n",
    "- - Khi đổi sang norm=l1, tổng các giá trị TF-IDF trong mỗi tài liệu sẽ bằng 1, giúp tập trung vào phân bố tần suất của các từ trong tài liệu thay vì độ lớn tuyệt đối của các vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Find words with the lowest and highest tf-idf values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams:\n",
      "Lowest 1-gram: 11doubledot55\n",
      "Highest 1-gram: viên\n",
      "TF-IDF with 1-gram:\n",
      "Lowest 1-gram TF-IDF: lõng\n",
      "Highest 1-gram TF-IDF: viên\n",
      "TF-IDF with 1-gram (new hyperparameters):\n",
      "Lowest 1-gram TF-IDF: vạ\n",
      "Highest 1-gram TF-IDF: dạy\n"
     ]
    }
   ],
   "source": [
    "# 1-ngram\n",
    "print(\"N-grams:\")\n",
    "print(\"Lowest 1-gram:\", word_frequency_1ngram.index[-1])\n",
    "print(\"Highest 1-gram:\", word_frequency_1ngram.index[0])\n",
    "print(\"TF-IDF with 1-gram:\")\n",
    "print(\"Lowest 1-gram TF-IDF:\", tfidf_sort_1gram.index[-1])\n",
    "print(\"Highest 1-gram TF-IDF:\", tfidf_sort_1gram.index[0])\n",
    "print(\"TF-IDF with 1-gram (new hyperparameters):\")\n",
    "print(\"Lowest 1-gram TF-IDF:\", new_tfidf_sort_1gram.index[-1])\n",
    "print(\"Highest 1-gram TF-IDF:\", new_tfidf_sort_1gram.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Highest 1-gram is similar with Highest 1-gram TF-IDF (\"viên\").\n",
    "- Lowest 1-gram is different from Lowest 1-gram TF-IDF.\n",
    "- Lowest 1-gram TF-IDF and Highest 1-gram TF-IDF of new hyperparameters are different from remainings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams:\n",
      "Lowest 2-gram: trên google\n",
      "Highest 2-gram: sinh viên\n",
      "TF-IDF with 2-gram:\n",
      "Lowest 2-gram TF-IDF: vạ gạo\n",
      "Highest 2-gram TF-IDF: nhiệt tình\n",
      "TF-IDF with 2-gram (new hyperparameters):\n",
      "Lowest 2-gram TF-IDF: gạo một\n",
      "Highest 2-gram TF-IDF: nhiệt tình\n"
     ]
    }
   ],
   "source": [
    "# 2-ngram\n",
    "print(\"N-grams:\")\n",
    "print(\"Lowest 2-gram:\", word_frequency_2ngram.index[-1])\n",
    "print(\"Highest 2-gram:\", word_frequency_2ngram.index[0])\n",
    "print(\"TF-IDF with 2-gram:\")\n",
    "print(\"Lowest 2-gram TF-IDF:\", tfidf_sort_2gram.index[-1])\n",
    "print(\"Highest 2-gram TF-IDF:\", tfidf_sort_2gram.index[0])\n",
    "print(\"TF-IDF with 2-gram (new hyperparameters):\")\n",
    "print(\"Lowest 2-gram TF-IDF:\", new_tfidf_sort_2gram.index[-1])\n",
    "print(\"Highest 2-gram TF-IDF:\", new_tfidf_sort_2gram.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lowest 2-gram and Highest 2-gram are different from remainings.\n",
    "- Highest 2-gram TF-IDF is similar with Highest 2-gram TF-IDF (new hyperparameters) (\"nhiệt tình\").\n",
    "- Lowest 2-gram TF-IDF is different from Lowest 2-gram TF-IDF (new hyperparameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Which limitations from n-grams that TF-IDF overcame ?\n",
    "- TF-IDF can overcome stopwords problem of n-grams:\n",
    "- - Khi sử dụng n-grams, stopwords như \"là\", \"của\", \"và\" vẫn có thể xuất hiện trong danh sách đặc trưng, làm giảm hiệu quả của mô hình.\n",
    "- - TF-IDF giúp giải quyết vấn đề này bằng cách giảm trọng số của các từ phổ biến trong toàn bộ tập dữ liệu, giúp loại bỏ những từ không quan trọng mà không cần danh sách stopwords cố định.\n",
    "- TF-IDF reduce weight of popular words:\n",
    "- - Trong một văn bản, những từ như \"tôi\", \"bạn\", \"có\" thường xuất hiện nhiều nhưng không mang nhiều ý nghĩa.\n",
    "- - TF-IDF giảm ảnh hưởng của những từ này bằng cách sử dụng giá trị IDF (Inverse Document Frequency), giúp tập trung vào các từ có ý nghĩa phân biệt trong tập dữ liệu.\n",
    "- TF-IDF separate important words from table, reduce size of useful data:\n",
    "- - N-grams có thể tạo ra một số lượng lớn đặc trưng, khiến mô hình trở nên cồng kềnh.\n",
    "- - TF-IDF chỉ giữ lại các từ ngữ có giá trị phân biệt cao, giúp giảm kích thước tập dữ liệu mà vẫn giữ được thông tin quan trọng.\n",
    "- TF-IDF evaluate the importance of dataset:\n",
    "- - TF-IDF không chỉ đo lường tần suất xuất hiện của một từ mà còn xem xét mức độ quan trọng của nó so với toàn bộ tập văn bản.\n",
    "- - Điều này giúp xác định các từ khóa quan trọng và cải thiện hiệu suất của các mô hình xử lý ngôn ngữ tự nhiên."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
