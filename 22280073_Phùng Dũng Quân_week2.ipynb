{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbWNBFv5ineh"
      },
      "source": [
        "# Word embedding and one-hot encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfCcod1xoDoF"
      },
      "source": [
        "## One-hot encoding\n",
        "\n",
        "> One-hot encoding is the process of turning categorical factors into a numerical structure that machine learning algorithms can readily process. It functions by representing each category in a feature as a binary vector of 1s and 0s, with the vector's size equivalent to the number of potential categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tl5VgYs_ipju"
      },
      "outputs": [],
      "source": [
        "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVqliCz4njHW"
      },
      "source": [
        "### One-hot integer encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf3v1gLInkwK",
        "outputId": "eb8d5f59-0864-43fe-83c5-52983f6cc01b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "[0 0 2 0 1 1 2 0 2 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(np.array(data))\n",
        "\n",
        "print(data)\n",
        "print(integer_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_eLAP-Mn9Dp"
      },
      "source": [
        "### One-hot binary encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8zKywxqn7-8",
        "outputId": "29aba087-401b-42c4-af70-6a16c21bdf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded_data = one_hot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "print(data)\n",
        "print(onehot_encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj-KxAzsyN67"
      },
      "source": [
        "## Problem 1\n",
        "What are the limitations of one-hot encoding?\n",
        "\n",
        "- Kích thước lớn, làm tăng bộ nhớ và thời gian xử lí:\n",
        "- - Nếu một cột có 10.000 giá trị khác nhau, One-Hot Encoding sẽ tạo ra 10.000 cột mới, gây lãng phí tài nguyên.\n",
        "- Ma trận thưa thớt:\n",
        "- - Hầu hết các giá trị trong vector One-Hot là 0, dẫn đến ma trận thưa.\n",
        "- Không thể biểu diễn mối quan hệ ngữ nghĩa giữa các từ:\n",
        "- - One-Hot Encoding coi tất cả các giá trị là độc lập và không có mối quan hệ nào giữa chúng.\n",
        "- - Từ \"vui\" và \"hạnh phúc\" có ý nghĩa gần nhau, nhưng vector của chúng lại hoàn toàn khác nhau.\n",
        "- Không thể xử lí tính phong phú của ngữ nghĩa:\n",
        "- - Một từ có thể có nhiều nghĩa khác nhau tùy theo ngữ cảnh, nhưng One-Hot Encoding không thể phân biệt được.\n",
        "- - One-Hot Encoding không cung cấp thông tin về ngữ cảnh của từ.\n",
        "- Không thể biểu diễn thông tin thứ tự của từ trong câu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['vui', 'hạnh phúc', 'buồn', 'tốt', 'xấu', 'vui', 'buồn', 'hạnh phúc']\n",
            "[3 1 0 2 4 3 0 1]\n",
            "['vui', 'hạnh phúc', 'buồn', 'tốt', 'xấu', 'vui', 'buồn', 'hạnh phúc']\n",
            "[[0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "words = ['vui', 'hạnh phúc', 'buồn', 'tốt', 'xấu', 'vui', 'buồn', 'hạnh phúc']\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(np.array(words))\n",
        "print(words)\n",
        "print(integer_encoded)\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "integer_encoded = integer_encoded.reshape(len(words), 1)\n",
        "onehot_encoded_data = one_hot_encoder.fit_transform(integer_encoded)\n",
        "print(words)\n",
        "print(onehot_encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7OsPVqbrbqr"
      },
      "source": [
        "## Word embedding\n",
        "\n",
        "ELI5 for word embeddings\n",
        "> The word embeddings can be thought of as a child’s understanding of the words. Initially, the word embeddings are randomly initialized and they don’t make any sense, just like the baby has no understanding of different words. It’s only after the model has started getting trained, the word vectors/embeddings start to capture the meaning of the words, just like the baby hears and learns different words.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "df8-lpXbtHe2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "W2EvpsPVvgmW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfvwcF5Uz_-A"
      },
      "source": [
        "### Unigram transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vPNwu0lWymvJ"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "from typing import List\n",
        "\n",
        "def ngrams_transform(document: List[str],\n",
        "                     n_gram: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    N-grams transformations for a given text\n",
        "\n",
        "    Args:\n",
        "    document (List[str]) -- The document to-be-processed\n",
        "    n_gram   (int)       -- Number of grams\n",
        "\n",
        "    Returns:\n",
        "    A list of string after n-grams processed\n",
        "    \"\"\"\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "    \n",
        "    tokenized_corpus = []\n",
        "    for sentence in corpus:\n",
        "        tokenized_sentence = []\n",
        "        for word in sentence.split():\n",
        "            tokenized_sentence.append(word.lower())\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    ngram_list = []\n",
        "    for sentence in tokenized_corpus:\n",
        "        for grams in ngrams(sentence, n_gram):\n",
        "            ngram_list.append(' '.join(grams))\n",
        "\n",
        "    return ngram_list\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKPIUAu_-gX5",
        "outputId": "471541d3-16f4-47ee-ddcc-f471fdc57566"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'document.',\n",
              " 'this',\n",
              " 'document',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'document.',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'third',\n",
              " 'one.',\n",
              " 'is',\n",
              " 'this',\n",
              " 'the',\n",
              " 'first',\n",
              " 'document?']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_grams_list = ngrams_transform(corpus,\n",
        "                                n_gram=1)\n",
        "n_grams_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w99wG6cn0Dap",
        "outputId": "37ee7987-7eaf-4ba1-b65f-754e380e343f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example text tensor: tensor([10,  5,  8,  4,  2, 10,  1,  5,  8,  7,  2,  0, 10,  5,  8,  9,  6,  5,\n",
            "        10,  8,  4,  3])\n",
            "Shape of example text tensor: torch.Size([22])\n"
          ]
        }
      ],
      "source": [
        "# Integer label for the given corpus\n",
        "label_encoder = LabelEncoder()\n",
        "corpus_vector = label_encoder.fit_transform(np.array(n_grams_list))\n",
        "\n",
        "# Tensorize the input vector\n",
        "example_text_tensor = torch.Tensor(corpus_vector).to(dtype=torch.long)\n",
        "print(f\"Example text tensor: {example_text_tensor}\")\n",
        "print(f\"Shape of example text tensor: {example_text_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uVmzQtLvs66"
      },
      "source": [
        "### Create an example for embedding function to map from a word dimension to a lower dimensional space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OJTdk0_drc0L"
      },
      "outputs": [],
      "source": [
        "num_vocab = 22 # number of vocabulary\n",
        "num_dimension = 50 # dimensional embeddings\n",
        "\n",
        "# Declare the mapping function\n",
        "example_embedding_function = nn.Embedding(num_vocab, num_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmTjA3-4uGKG",
        "outputId": "29df29ce-6427-4231-9eb7-b2132703adbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: torch.Size([22, 50])\n"
          ]
        }
      ],
      "source": [
        "example_output_tensor = example_embedding_function(example_text_tensor)\n",
        "print(f\"Embedding shape: {example_output_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oElUUcX2ZyDX"
      },
      "source": [
        "# Word2vec\n",
        "\n",
        "\n",
        "* Word2vec is a **class of models** that represents a word in a large text corpus as a vector in n-dimensional space(or n-dimensional feature space) bringing similar words closer to each other.\n",
        "\n",
        "\n",
        "\n",
        "* Word2vec is a simple yet popular model to construct representating embedding for words from a representation space to a much lower dimensional space (compared to the respective number of words in a dictionary).\n",
        "\n",
        "\n",
        "\n",
        "* Word2Vec has two neural network-based variants, which are:\n",
        "\n",
        "    * Continuous Bag of Words (CBOW)\n",
        "    * Skip-gram.\n",
        "![](https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-2048x1075.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwAVXC0V8vWA"
      },
      "source": [
        "## Continuous Bag of words (CBOW)\n",
        "\n",
        "* The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typically, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
        "\n",
        "* CBOW is modelled as follows:\n",
        "    * Given a target word $w_i$ and an $N$ context window on each side, $w_{i-1}, \\cdots, w_{i-N}$ and $w_{i+1},\\cdots, w_{i+N}$, referring to all context words collectively as $C$.\n",
        "\n",
        "    * CBOW tries to minimize the objective function:\n",
        "\n",
        "$$\n",
        "-\\log p(w_i|C) = -\\log\\text{Softmax}\\left(A\\left(\\sum_{w\\in C}q_w\\right)+b\\right)\n",
        "$$\n",
        "\n",
        "where $q_w$ is the embedding of word $w$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSvo8dncDUvv",
        "outputId": "b0d917d9-fe82-4a1d-dc5e-b6d1039bab4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# N = 2 according to the definition\n",
        "CONTEXT_SIZE = 2\n",
        "\n",
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "corpus = corpus.split()\n",
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3MOE0WmFBd-"
      },
      "source": [
        "### Create an integer mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4eHuiF8yOj",
        "outputId": "4ffbd020-0616-4a35-d4d5-a71fcafe578f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rules': 0,\n",
              " 'In': 1,\n",
              " 'we': 2,\n",
              " 'they': 3,\n",
              " 'idea': 4,\n",
              " 'data.': 5,\n",
              " 'computers.': 6,\n",
              " 'computational': 7,\n",
              " 'are': 8,\n",
              " 'spirits': 9,\n",
              " 'The': 10,\n",
              " 'computer': 11,\n",
              " 'that': 12,\n",
              " 'to': 13,\n",
              " 'spells.': 14,\n",
              " 'process': 15,\n",
              " 'People': 16,\n",
              " 'direct': 17,\n",
              " 'Computational': 18,\n",
              " 'process.': 19,\n",
              " 'inhabit': 20,\n",
              " 'of': 21,\n",
              " 'manipulate': 22,\n",
              " 'effect,': 23,\n",
              " 'other': 24,\n",
              " 'study': 25,\n",
              " 'beings': 26,\n",
              " 'programs': 27,\n",
              " 'about': 28,\n",
              " 'pattern': 29,\n",
              " 'program.': 30,\n",
              " 'processes': 31,\n",
              " 'the': 32,\n",
              " 'by': 33,\n",
              " 'evolve,': 34,\n",
              " 'with': 35,\n",
              " 'We': 36,\n",
              " 'a': 37,\n",
              " 'is': 38,\n",
              " 'conjure': 39,\n",
              " 'our': 40,\n",
              " 'things': 41,\n",
              " 'evolution': 42,\n",
              " 'directed': 43,\n",
              " 'processes.': 44,\n",
              " 'abstract': 45,\n",
              " 'As': 46,\n",
              " 'called': 47,\n",
              " 'create': 48}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Integer word mapping\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "word_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqrPWRsTE5zA"
      },
      "source": [
        "### Build context according to the given corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgBAHKy5CoJ7",
        "outputId": "7f8fa471-f3e3-40cf-bf52-fa47c2607486"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(['are', 'We', 'to', 'study'], 'about'),\n",
              " (['about', 'are', 'study', 'the'], 'to'),\n",
              " (['to', 'about', 'the', 'idea'], 'study'),\n",
              " (['study', 'to', 'idea', 'of'], 'the'),\n",
              " (['the', 'study', 'of', 'a'], 'idea'),\n",
              " (['idea', 'the', 'a', 'computational'], 'of'),\n",
              " (['of', 'idea', 'computational', 'process.'], 'a'),\n",
              " (['a', 'of', 'process.', 'Computational'], 'computational'),\n",
              " (['computational', 'a', 'Computational', 'processes'], 'process.'),\n",
              " (['process.', 'computational', 'processes', 'are'], 'Computational'),\n",
              " (['Computational', 'process.', 'are', 'abstract'], 'processes'),\n",
              " (['processes', 'Computational', 'abstract', 'beings'], 'are'),\n",
              " (['are', 'processes', 'beings', 'that'], 'abstract'),\n",
              " (['abstract', 'are', 'that', 'inhabit'], 'beings'),\n",
              " (['beings', 'abstract', 'inhabit', 'computers.'], 'that'),\n",
              " (['that', 'beings', 'computers.', 'As'], 'inhabit'),\n",
              " (['inhabit', 'that', 'As', 'they'], 'computers.'),\n",
              " (['computers.', 'inhabit', 'they', 'evolve,'], 'As'),\n",
              " (['As', 'computers.', 'evolve,', 'processes'], 'they'),\n",
              " (['they', 'As', 'processes', 'manipulate'], 'evolve,'),\n",
              " (['evolve,', 'they', 'manipulate', 'other'], 'processes'),\n",
              " (['processes', 'evolve,', 'other', 'abstract'], 'manipulate'),\n",
              " (['manipulate', 'processes', 'abstract', 'things'], 'other'),\n",
              " (['other', 'manipulate', 'things', 'called'], 'abstract'),\n",
              " (['abstract', 'other', 'called', 'data.'], 'things'),\n",
              " (['things', 'abstract', 'data.', 'The'], 'called'),\n",
              " (['called', 'things', 'The', 'evolution'], 'data.'),\n",
              " (['data.', 'called', 'evolution', 'of'], 'The'),\n",
              " (['The', 'data.', 'of', 'a'], 'evolution'),\n",
              " (['evolution', 'The', 'a', 'process'], 'of'),\n",
              " (['of', 'evolution', 'process', 'is'], 'a'),\n",
              " (['a', 'of', 'is', 'directed'], 'process'),\n",
              " (['process', 'a', 'directed', 'by'], 'is'),\n",
              " (['is', 'process', 'by', 'a'], 'directed'),\n",
              " (['directed', 'is', 'a', 'pattern'], 'by'),\n",
              " (['by', 'directed', 'pattern', 'of'], 'a'),\n",
              " (['a', 'by', 'of', 'rules'], 'pattern'),\n",
              " (['pattern', 'a', 'rules', 'called'], 'of'),\n",
              " (['of', 'pattern', 'called', 'a'], 'rules'),\n",
              " (['rules', 'of', 'a', 'program.'], 'called'),\n",
              " (['called', 'rules', 'program.', 'People'], 'a'),\n",
              " (['a', 'called', 'People', 'create'], 'program.'),\n",
              " (['program.', 'a', 'create', 'programs'], 'People'),\n",
              " (['People', 'program.', 'programs', 'to'], 'create'),\n",
              " (['create', 'People', 'to', 'direct'], 'programs'),\n",
              " (['programs', 'create', 'direct', 'processes.'], 'to'),\n",
              " (['to', 'programs', 'processes.', 'In'], 'direct'),\n",
              " (['direct', 'to', 'In', 'effect,'], 'processes.'),\n",
              " (['processes.', 'direct', 'effect,', 'we'], 'In'),\n",
              " (['In', 'processes.', 'we', 'conjure'], 'effect,'),\n",
              " (['effect,', 'In', 'conjure', 'the'], 'we'),\n",
              " (['we', 'effect,', 'the', 'spirits'], 'conjure'),\n",
              " (['conjure', 'we', 'spirits', 'of'], 'the'),\n",
              " (['the', 'conjure', 'of', 'the'], 'spirits'),\n",
              " (['spirits', 'the', 'the', 'computer'], 'of'),\n",
              " (['of', 'spirits', 'computer', 'with'], 'the'),\n",
              " (['the', 'of', 'with', 'our'], 'computer'),\n",
              " (['computer', 'the', 'our', 'spells.'], 'with')]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = []\n",
        "\n",
        "for i in range(CONTEXT_SIZE, len(corpus) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [corpus[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [corpus[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = corpus[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbFTMNAGWygb"
      },
      "source": [
        "### Problem 2\n",
        "Name at least 2 limitations at this context construction step? Explain your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Mất trật tự của ngữ cảnh:\n",
        "- Khi ghép hai danh sách (trước và sau từ trung tâm), thứ tự thực tế của các từ trong câu có thể bị đảo lộn, khiến mô hình khó học được cú pháp chính xác. Làm mất đi ý nghĩa ngữ pháp của câu.\n",
        "2. Các từ nằm vị trí xa bị bỏ qua:\n",
        "- Phương pháp trên chỉ lấy 1 số từ cố định trước hoặc sau từ target, điều này khiến cho các từ nằm xa có thể bị bỏ qua dù mang ý nghĩa quan trọng.\n",
        "- Ví dụ: \"Bạn Tuấn, dù nhà ở xa nhưng vẫn luôn đi học đúng giờ.\". Cụm từ \"luôn đi học đúng giờ\" trong câu trên mới mang ý chính của câu. Tuy nhiên, nếu ta chọn target word là \"Tuấn\" thì CBOW chỉ dự đoán được \"Bạn\", \"dù\", \"nhà\" và không nắm bắt được ý chính của câu.\n",
        "3. Không xử lý được từ đa nghĩa:\n",
        "- Ví dụ: \"Tôi không thích uống nước đá.\" và \"Tôi thích đá bóng.\". Từ \"đá\" trong 2 câu trên mang nghĩa hoàn toàn khác nhau nhưng CBOW chỉ học duy nhất vector \"đá\" nên không thể phân biệt 2 nghĩa này.\n",
        "4. Lặp lại target nhiều lần khiến cho mô hình dễ bị overfit gây ra việc khó phân biệt ngữ nghĩa thực tế."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYMlC5FSHSqP"
      },
      "source": [
        "### Vectorize context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9aRCDPmYEnMg"
      },
      "outputs": [],
      "source": [
        "def make_context_vector(context: List[str],\n",
        "                        word_to_idx: dict) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Function to map a word context vector into a torch tensor\n",
        "\n",
        "    Args:\n",
        "    context (List[str]) -- A context (including individual n-grams tokens)\n",
        "    word_to_idx (dict)  -- A functionto map a word into its respective integer\n",
        "\n",
        "    Returns:\n",
        "    A pytorch tensor including a list of mapped word\n",
        "\n",
        "    Example:\n",
        "    ['are', 'We', 'to', 'study'] --> tensor([40, 22, 27, 47])\n",
        "    \"\"\"\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "    list_idx = []\n",
        "    for word in context:\n",
        "        if word in word_to_idx:\n",
        "            list_idx.append(word_to_idx[word])\n",
        "\n",
        "    return torch.tensor(list_idx, dtype=torch.long)\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmf-xZkEFOfZ",
        "outputId": "96784ab6-8c41-4bdf-dacb-f23245314411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sample:  ['are', 'We', 'to', 'study']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([ 8, 36, 13, 25])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Functional test\n",
        "print(\"Example sample: \", data[0][0])\n",
        "make_context_vector(data[0][0], word_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h_4P68vHVOW"
      },
      "source": [
        "### CBOW model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ViLponY1HZoD"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Neural weight initialization\n",
        "        nn.init.xavier_normal_(self.embedding_layer.weight)\n",
        "        nn.init.xavier_normal_(self.linear_layer.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Function to conduct forward passing\n",
        "        \"\"\"\n",
        "        embedding = self.embedding_layer(inputs)\n",
        "        embedding = torch.sum(embedding, dim=1)\n",
        "        output = self.linear_layer(embedding)\n",
        "        output_softmax = F.log_softmax(output, dim=1)\n",
        "        return output_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_vTUt8aJiEn",
        "outputId": "d9600ad7-9c9e-48b1-8bb9-be49a4863c9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CBOW(\n",
              "  (embedding_layer): Embedding(49, 10)\n",
              "  (linear_layer): Linear(in_features=10, out_features=49, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cbow_model = CBOW(vocab_size=vocab_size,\n",
        "                  embed_dim=10)\n",
        "\n",
        "# Enable gradient for model training\n",
        "cbow_model.train()\n",
        "cbow_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9aXrzRCKQel"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5tYJoeKMmuz"
      },
      "source": [
        "#### Hyperparameters and training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UxbOAYCDLlrd"
      },
      "outputs": [],
      "source": [
        "num_epochs: int = 5\n",
        "learning_rate: float = 5e-2\n",
        "optimizer: torch.optim = torch.optim.Adam(cbow_model.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "loss_function = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgn6aHDsWk2r"
      },
      "source": [
        "#### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bv0CVQZKS7W",
        "outputId": "9670abaf-b513-460f-c129-0764a38009a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Epoch 1/5\n",
            "Loss: 3.884511709213257\n",
            "#Epoch 2/5\n",
            "Loss: 3.112879753112793\n",
            "#Epoch 3/5\n",
            "Loss: 2.1810035705566406\n",
            "#Epoch 4/5\n",
            "Loss: 1.1467602252960205\n",
            "#Epoch 5/5\n",
            "Loss: 0.34934601187705994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4052\\2192199030.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4052\\2192199030.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"#Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Construct input and target tensor\n",
        "    input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
        "    input_vector = input_vector.unsqueeze(0)\n",
        "    target_vector = target_vector.unsqueeze(0)\n",
        "\n",
        "    # Join whole data into 1 tensor set\n",
        "    for idx in range(1, len(data)):\n",
        "        input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n",
        "        target_tensor = torch.tensor(word_to_idx[data[idx][1]]).unsqueeze(0)\n",
        "        torch.cat((input_vector, input_tensor), 0)\n",
        "        torch.cat((target_vector, target_tensor), 0)\n",
        "\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "    cbow_model.zero_grad()\n",
        "\n",
        "    # Forward passing\n",
        "    log_probabilities = cbow_model(input_vector)\n",
        "\n",
        "    # Evaluate loss\n",
        "    loss = loss_function(log_probabilities, target_vector)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = loss.item()\n",
        "    print(\"Loss:\", epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DrF_htiSXmI"
      },
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3SBZo4LSDP5",
        "outputId": "de69eb87-1f11-42a8-a44f-5c2d0d3d12f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: ['In', 'processes.', 'we', 'conjure']\n",
            "Prediction: about\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4052\\1681364246.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # No gradient update in inference\n",
        "    context = ['In', 'processes.', 'we', 'conjure']\n",
        "\n",
        "    # Vectorize input from text to numeric type\n",
        "    input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n",
        "\n",
        "    # Model makes prediction\n",
        "    output_tensor = cbow_model(input_tensor)\n",
        "\n",
        "    # Get the item id with the highest probability\n",
        "    prediction = torch.argmax(output_tensor).detach().tolist()\n",
        "\n",
        "    # Query the respective word from the given item id\n",
        "    key_list = list(word_to_idx.keys())\n",
        "    prediction = key_list[prediction]\n",
        "\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9FwHWmGaM3S"
      },
      "source": [
        "## Skip-gram\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningcoban.com/tabml_book/_images/word2vec2.png\">\n",
        "</center>\n",
        "\n",
        "- Skip gram is based on the distributional hypothesis where words with similar distribution is considered to have similar meanings. Researchers of skip gram suggested a model with less parameters along with the novel methods to make optimization step more efficient.\n",
        "\n",
        "- Vanilla SkipGram model:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/a1d083c872e848787cb572a73d97f2c24947a374/5-Figure1-1.png\" scale=70%>\n",
        "</center>\n",
        "\n",
        "- Main idea is to optimize model so that if it is queried with a word, it should correctly guess all the context (context = 2 in the figure) words. That is,\n",
        "$$\n",
        "y=\\sigma(Ux)\n",
        "$$\n",
        "    - where $x$, $y$ are one-hot encoded word vector, $U$ is the embedding matrix, and $\\sigma(\\cdot)$ is the softmax function.\n",
        "\n",
        "With the same dataset, training set for skip gram can be much larger than that of NPLM since it can have $2c$ samples $\\left(w_t:w_{t-c}, ...,w_t:w_{t-1},w_t:w_{t+1},...,w_{t+c}\\right)$ while other n-gram based models have one $\\left((w_{t-c},...w_{t-1},w_{t+1},...,w_{t+c}):w_t\\right)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "b9XRCWEvEOWC"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "S1dwgXNxZ_Lv"
      },
      "outputs": [],
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model construction\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        ### START YOUR CODE HERE ###\n",
        "        # Declare embedding function u and v\n",
        "        # with given vocab size and embed dim using nn.Embedding\n",
        "        self.v_embedding_layer = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        self.u_embedding_layer = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "\n",
        "        # Network weight initialization with Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.v_embedding_layer.weight)\n",
        "        nn.init.xavier_uniform_(self.u_embedding_layer.weight)\n",
        "\n",
        "        ### END YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, center_words, context):\n",
        "        \"\"\"\n",
        "        Function to perform forward passing\n",
        "        \"\"\"\n",
        "        v_embedding = self.v_embedding_layer(center_words)\n",
        "        u_embedding = self.u_embedding_layer(context)\n",
        "\n",
        "        score = torch.mul(v_embedding, u_embedding)\n",
        "        score = torch.sum(score, dim=1)\n",
        "        log_score = F.logsigmoid(score)\n",
        "        return log_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ls_wI-7nS1",
        "outputId": "f69f5496-261f-48c2-8ead-f4b20dca100f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SkipGramModel(\n",
              "  (v_embedding_layer): Embedding(49, 128)\n",
              "  (u_embedding_layer): Embedding(49, 128)\n",
              ")"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "skipgram_model = SkipGramModel(vocab_size=vocab_size,\n",
        "                               embed_dim=128)\n",
        "\n",
        "skipgram_model.train()\n",
        "skipgram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiBCaJJJCMr_"
      },
      "source": [
        "### Prepare training data to match the format of SkipGram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "5YlRPTuaCQBS"
      },
      "outputs": [],
      "source": [
        "def gather_training_data(corpus,\n",
        "                         word_to_idx: dict,\n",
        "                         context_size: int):\n",
        "    \"\"\"\n",
        "    This function is to transform the given corpus\n",
        "    into the correct format for SkipGram to serve as its input\n",
        "    \"\"\"\n",
        "\n",
        "    training_data = []\n",
        "    all_vocab_indices = list(range(len(word_to_idx)))\n",
        "\n",
        "    split_text = corpus.split('\\n')\n",
        "\n",
        "    # For each sentence\n",
        "    for sentence in split_text:\n",
        "        indices = []\n",
        "        indices = [word_to_idx[word] for word in sentence.split(' ')]\n",
        "\n",
        "        # For each word treated as center word\n",
        "        for center_word_pos in range(len(indices)):\n",
        "\n",
        "            # For each window  position\n",
        "            for w in range(-context_size, context_size+1):\n",
        "                context_word_pos = center_word_pos + w\n",
        "\n",
        "                # Make sure we dont jump out of the sentence\n",
        "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                    continue\n",
        "\n",
        "                context_word_idx = indices[context_word_pos]\n",
        "                center_word_idx  = indices[center_word_pos]\n",
        "\n",
        "                # Same words might be present in the close vicinity of each other. we want to avoid such cases\n",
        "                if center_word_idx == context_word_idx:\n",
        "                    continue\n",
        "\n",
        "                training_data.append([center_word_idx, context_word_idx])\n",
        "\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVjk1oJkDPXq",
        "outputId": "5c6aa12d-ea15-4ced-8254-c0d2d0687714"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([212, 2])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data = gather_training_data(corpus,\n",
        "                                     word_to_idx,\n",
        "                                     context_size=2)\n",
        "training_data = torch.tensor(training_data).to(dtype=torch.long)\n",
        "training_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWEIXRlg8zmg"
      },
      "source": [
        "### Hyperparamters and training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "wIgEYDKz8Sbu"
      },
      "outputs": [],
      "source": [
        "num_epochs: int = 200\n",
        "learning_rate: float = 5e-1\n",
        "optimizer: torch.optim = torch.optim.SGD(skipgram_model.parameters(),\n",
        "                                          lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmG0m3jK84EF"
      },
      "source": [
        "### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhGy9vBq85PX",
        "outputId": "fe2241aa-0ede-41d4-f0b0-91fcec8aa755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Epoch 0/200\n",
            "Loss: 0.6899917721748352\n",
            "#Epoch 50/200\n",
            "Loss: 0.5998015403747559\n",
            "#Epoch 100/200\n",
            "Loss: 0.5147236585617065\n",
            "#Epoch 150/200\n",
            "Loss: 0.4317810833454132\n",
            "#Epoch 200/200\n",
            "Loss: 0.35444021224975586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4052\\167642886.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(training_data[:, 0], dtype=torch.long)\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4052\\167642886.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target_tensor = torch.tensor(training_data[:, 1], dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs + 1):\n",
        "    \"\"\"\n",
        "    Adapt the given CBOW training code for SkipGram\n",
        "    Following by the instruction comments, or you could do it on your own ;)\n",
        "    \"\"\"\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "    # Construct input and target tensor\n",
        "    input_tensor = torch.tensor(training_data[:, 0], dtype=torch.long)\n",
        "    target_tensor = torch.tensor(training_data[:, 1], dtype=torch.long)\n",
        "\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "    skipgram_model.zero_grad()\n",
        "\n",
        "    # Forward passing\n",
        "    logsoftmax_prediction = skipgram_model(input_tensor, target_tensor)\n",
        "\n",
        "    # Evaluate loss (Negative log likelihood)\n",
        "    loss = torch.mean(-1 * logsoftmax_prediction)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = loss.item()\n",
        "\n",
        "    # Log result\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"#Epoch {epoch}/{num_epochs}\")\n",
        "        print(\"Loss:\", epoch_loss)\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8DX0BfFDsK"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "U2EcHAqVFEzb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: ['we']\n",
            "Prediction: ['are', 'study', 'of', 'spirits']\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    context = ['we']\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Based on the given inference code in the previous section, training code and the context\n",
        "    # Implement the inference flow from the given context to an output word\n",
        "\n",
        "    embedding_matrix = skipgram_model.v_embedding_layer.weight\n",
        "    input_tensor = torch.tensor(word_to_idx[context[0]]).unsqueeze(0)\n",
        "    input_embedding = embedding_matrix[input_tensor]\n",
        "    predict_matrix = F.log_softmax(input_embedding @ embedding_matrix.T, dim=1)\n",
        "    context_size = 2\n",
        "    top_indices = torch.topk(predict_matrix, 2 * context_size + 1, sorted=True).indices\n",
        "    key_list = list(word_to_idx.keys())\n",
        "    prediction = [key_list[idx] for idx in top_indices[0]]\n",
        "    prediction = prediction[1:]\n",
        "\n",
        "    ### END YOUR CODE HERE ###\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sg4EaBpw-De"
      },
      "source": [
        "## Problem 3\n",
        "What are the differences between CBOW and Skip-gram?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. CBOW\n",
        "- Dự đoán từ trung tâm (target word) dựa vào các từ ngữ cảnh (context words).\n",
        "- Hiệu quả với tập dữ liệu lớn, tốc độ nhanh hơn do số lượng mẫu huấn luyện ít hơn.\n",
        "- Dữ liệu nhỏ, tốc độ huấn luyện nhanh, hoạt động tốt với từ phổ biến.\n",
        "\n",
        "2. Skip-gram\n",
        "- Dự đoán các từ ngữ cảnh dựa vào từ trung tâm.\n",
        "- Hiệu quả với tập dữ liệu nhỏ, có thể học tốt hơn trong trường hợp dữ liệu thưa thớt nhưng tốc độ chậm.\n",
        "- Dữ liệu lớn, hiệu quả hơn với từ hiếm."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
